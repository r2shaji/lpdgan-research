{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e50c6419-d346-4a14-a372-45b277b7a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(image_path, label_dir_path):\n",
    "    label_file_name = os.path.basename(image_path).split(\".\")[0] + \".txt\"\n",
    "    label_file_path = os.path.join(label_dir_path,label_file_name)\n",
    "    with open(label_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    lines= [line.rstrip() for line in lines]\n",
    "    lines= [line.split() for line in lines]\n",
    "    lines = numpy.array(lines).astype(float)\n",
    "    lines = torch.from_numpy(lines)\n",
    "    lines = sorted(lines, key=lambda x: x[1])\n",
    "    sorted_labels = torch.tensor([t[0] for t in lines], dtype=torch.float64)\n",
    "    sorted_boxes = [t[1:] for t in lines]\n",
    "    plate_info = { \"sorted_labels\": sorted_labels, \"sorted_boxes_xywhn\":sorted_boxes}\n",
    "    return plate_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07b6551d-402e-453d-8a93-b2f5811e2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "lines = load_label(r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp\\2a602f33-112d-4003-9580-cdb52dbdc245_52_737_gt_3281721_roi_3281721_151.jpg\",r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "545b9a4d-d081-48c2-847c-f8de1f2bb2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sorted_labels': tensor([ 3., 18.,  2., 23.,  8., 13., 18.,  1., 10.,  7., 23.,  2., 10.,  1.],\n",
       "        dtype=torch.float64),\n",
       " 'sorted_boxes_xywhn': [tensor([0.2407, 0.4377, 0.0825, 0.3378], dtype=torch.float64),\n",
       "  tensor([0.3193, 0.1722, 0.0264, 0.1111], dtype=torch.float64),\n",
       "  tensor([0.3329, 0.4566, 0.0816, 0.3311], dtype=torch.float64),\n",
       "  tensor([0.3743, 0.1761, 0.0676, 0.1122], dtype=torch.float64),\n",
       "  tensor([0.4207, 0.4716, 0.0830, 0.3389], dtype=torch.float64),\n",
       "  tensor([0.4554, 0.1922, 0.0716, 0.1178], dtype=torch.float64),\n",
       "  tensor([0.5061, 0.2056, 0.0297, 0.1133], dtype=torch.float64),\n",
       "  tensor([0.5083, 0.4916, 0.0878, 0.3344], dtype=torch.float64),\n",
       "  tensor([0.5639, 0.2150, 0.0682, 0.1167], dtype=torch.float64),\n",
       "  tensor([0.6032, 0.5021, 0.0818, 0.3378], dtype=torch.float64),\n",
       "  tensor([0.6422, 0.2256, 0.0682, 0.1222], dtype=torch.float64),\n",
       "  tensor([0.6880, 0.5221, 0.0878, 0.3422], dtype=torch.float64),\n",
       "  tensor([0.7122, 0.2439, 0.0635, 0.1211], dtype=torch.float64),\n",
       "  tensor([0.7688, 0.5354, 0.0736, 0.3422], dtype=torch.float64)]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a18412-e8dd-40e1-85b3-ef9c0dc49d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "\n",
    "model = YOLO(r\"D:\\Users\\r2shaji\\Downloads\\ocr_detection_model_v7_sz512_20241102\\ocr_detection_model_v7_sz512_20241102.onnx\")  \n",
    "image = cv2.imread(r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp\\2a602f33-112d-4003-9580-cdb52dbdc245_52_737_gt_3281721_roi_3281721_151.jpg\")\n",
    "fake_B_PlateNum = model.predict(image)\n",
    "\n",
    "plate_info = load_label(r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp\\2a602f33-112d-4003-9580-cdb52dbdc245_52_737_gt_3281721_roi_3281721_151.jpg\",r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\label\")\n",
    "sorted_fake_B_PlateNum = sort_bbox(fake_B_PlateNum[0].boxes.xywhn)\n",
    "\n",
    "\n",
    "print('self.plate_info[\"sorted_boxes_xywhn\"]:',plate_info[\"sorted_boxes_xywhn\"])\n",
    "print('sorted_fake_B_PlateNum:',sorted_fake_B_PlateNum)\n",
    "\n",
    "\n",
    "loss_CIoU_Plate = get_loss(sorted_fake_B_PlateNum,plate_info[\"sorted_boxes_xywhn\"])\n",
    "print(loss_CIoU_Plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7098fc26-f9eb-464d-89c3-228c9bc47c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "lines = np.array(lines).astype(float)\n",
    "lines = torch.from_numpy(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c303a5f0-b090-42b8-9d78-eadf55e5fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([3.0000, 0.2407, 0.4377, 0.0825, 0.3378], dtype=torch.float64),\n",
       " tensor([18.0000,  0.3193,  0.1722,  0.0264,  0.1111], dtype=torch.float64),\n",
       " tensor([2.0000, 0.3329, 0.4566, 0.0816, 0.3311], dtype=torch.float64),\n",
       " tensor([23.0000,  0.3743,  0.1761,  0.0676,  0.1122], dtype=torch.float64),\n",
       " tensor([8.0000, 0.4207, 0.4716, 0.0830, 0.3389], dtype=torch.float64),\n",
       " tensor([13.0000,  0.4554,  0.1922,  0.0716,  0.1178], dtype=torch.float64),\n",
       " tensor([18.0000,  0.5061,  0.2056,  0.0297,  0.1133], dtype=torch.float64),\n",
       " tensor([1.0000, 0.5083, 0.4916, 0.0878, 0.3344], dtype=torch.float64),\n",
       " tensor([10.0000,  0.5639,  0.2150,  0.0682,  0.1167], dtype=torch.float64),\n",
       " tensor([7.0000, 0.6032, 0.5021, 0.0818, 0.3378], dtype=torch.float64),\n",
       " tensor([23.0000,  0.6422,  0.2256,  0.0682,  0.1222], dtype=torch.float64),\n",
       " tensor([2.0000, 0.6880, 0.5221, 0.0878, 0.3422], dtype=torch.float64),\n",
       " tensor([10.0000,  0.7122,  0.2439,  0.0635,  0.1211], dtype=torch.float64),\n",
       " tensor([1.0000, 0.7688, 0.5354, 0.0736, 0.3422], dtype=torch.float64)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lines, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4bf1fa20-72c4-4a33-b910-33882b67333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_indices_target_boxes(pred_boxes, true_boxes, ciou_threshold=0.3):\n",
    "\n",
    "        max_loss = 1\n",
    "        if len(pred_boxes) == 0:\n",
    "            return max_loss\n",
    "\n",
    "        pred_boxes = torch.stack(pred_boxes)\n",
    "        true_boxes = torch.stack(true_boxes)\n",
    "        pred_boxes = pred_boxes.float().to(pred_boxes.device)\n",
    "        true_boxes = true_boxes.float().to(true_boxes.device)\n",
    "\n",
    "        matched_indices = {}\n",
    "\n",
    "        for idx, pred_box in enumerate(pred_boxes):\n",
    "            ciou_scores = bbox_iou(pred_box,true_boxes, xywh=True, CIoU=True)\n",
    "            best_ciou= ciou_scores.max()\n",
    "            matched_idx = torch.argmax(ciou_scores)\n",
    "            print(matched_idx)\n",
    "            if best_ciou > ciou_threshold:\n",
    "                matched_indices[idx] = matched_idx.item()\n",
    "\n",
    "        return matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "baed0e5b-ba33-48d2-8ff6-aa546fcac217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(4)\n",
      "tensor(7)\n",
      "tensor(9)\n",
      "tensor(11)\n",
      "tensor(13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 2, 2: 4, 3: 7, 4: 9, 5: 11, 6: 13}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_indices = select_indices_target_boxes(sorted_fake_B_PlateNum,plate_info[\"sorted_boxes_xywhn\"])\n",
    "matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "10a2802a-be8c-469c-ba56-b64604a3071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corresponding_correct_label(matched_indices,sorted_labels):\n",
    "    relevant_indices = list(matched_indices.values())\n",
    "    relevant_values = sorted_labels[relevant_indices]\n",
    "    return relevant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e526567d-c99b-4f44-97b7-8406951d6e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 7, 9, 11, 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 8., 1., 7., 2., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(matched_indices.values()))\n",
    "find_corresponding_correct_label(matched_indices,plate_info[\"sorted_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02efdbd9-869e-442a-911d-c0932fb6963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_log_prob(sorted_class_log_prob,matched_indices):\n",
    "        relevant_indices = list(matched_indices.keys())\n",
    "        relevant_values = sorted_class_log_prob[relevant_indices]\n",
    "        return relevant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbb5c3-e4e3-4e79-a6fa-25c7ec85abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_relevant_log_prob(sorted_class_log_prob,matched_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a065901-9b6e-4f0b-83cd-3e9adb97b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images(input_folder, output_folder, size=(256, 256)):\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output directory: {output_folder}\")\n",
    "\n",
    "    supported_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')\n",
    "    save_params = {}\n",
    "    save_params['quality'] = 100\n",
    "    save_params['subsampling'] = 0\n",
    "    save_params['optimize'] = False\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(supported_extensions):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            try:\n",
    "                with Image.open(input_path) as img:\n",
    "                    # Resize the image\n",
    "                    resized_img = img.resize(size)\n",
    "                    # Save the resized image to the output folder\n",
    "                    resized_img.save(output_path, **save_params)\n",
    "                    print(f\"Resized and saved: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "\n",
    "input_dir = r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp\"\n",
    "output_dir = r\"D:\\Users\\r2shaji\\Downloads\\lpdata\\ocr_merged\\train\\sharp_resized\"\n",
    "resize_images(input_dir, output_dir, size=(256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abfb8c-20cc-4196-bbbd-494cc57387c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
